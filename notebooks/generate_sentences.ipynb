{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../raw_data/cltk_json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from cltk import NLP \n",
    "from cltk.core.data_types import Word, Doc, Sentence\n",
    "from tqdm import tqdm\n",
    "# import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "grc_json_list = [f for f in os.listdir(DATA_DIR) if \"__grc.json\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_chars = [\"\\n\", \"\\t\"]\n",
    "def recursive_traversal(d: dict):\n",
    "    values = d.values()\n",
    "    out_strings = []\n",
    "    if all(type(v) is str for v in values):\n",
    "        out_strings.append(\" \".join(values))\n",
    "    else:\n",
    "        for value in values:\n",
    "            if type(value) is dict:\n",
    "                out_strings.extend(recursive_traversal(value))\n",
    "            else:\n",
    "                out_strings.append(value)\n",
    "    return out_strings\n",
    "\n",
    "def postprocess_string(t: str):\n",
    "    for r in removed_chars:\n",
    "        t = t.replace(r, \"\")\n",
    "    t = t\\\n",
    "        .replace(\"   \", \" \")\\\n",
    "        .replace(\"  \", \" \")\\\n",
    "        .replace(\"..\", \".\")\\\n",
    "        .strip()\\\n",
    "        .lstrip(\". \")\n",
    "    return t\n",
    "\n",
    "def get_file_texts(filename: str, dir: str=DATA_DIR):\n",
    "    full_path = f\"{DATA_DIR}/{filename}\"\n",
    "    file_json = json.load(open(full_path))\n",
    "    # print(file_json)\n",
    "    metadata = {\n",
    "        \"language\": file_json.get(\"language\", \"unknown\"),\n",
    "        \"title\": file_json.get(\"englishTitle\", file_json.get('work', '')),\n",
    "        \"urn\": file_json.get(\"urn\", \"\"),\n",
    "        \"author\": file_json.get(\"author\", \"\"),\n",
    "        \"edition\": file_json.get(\"edition\", \"\")\n",
    "    }\n",
    "    # print(f\"{file_json['englishTitle']} by {file_json['author']}\")\n",
    "    text_strings = recursive_traversal(file_json[\"text\"])\n",
    "    postprocessed_strings = list(map(postprocess_string, text_strings))\n",
    "    return postprocessed_strings, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_raw_text(s: Sentence):\n",
    "    sent_str = \" \".join([w.string for w in s.words])\n",
    "    sent_str = sent_str\\\n",
    "        .replace(\" ,\", \",\")\\\n",
    "        .replace(\" ·\", \"·\")\\\n",
    "        .replace(\" .\", \".\")\n",
    "    return sent_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [11:11<00:00,  3.04s/it]\n"
     ]
    }
   ],
   "source": [
    "df_sents_raw = pd.DataFrame(columns=(\"sentence\", \"metadata\"))\n",
    "grc_nlp = NLP(language=\"grc\", suppress_banner=True)\n",
    "for path in tqdm(grc_json_list):\n",
    "    try:\n",
    "        texts, metadata = get_file_texts(path)\n",
    "        metadata[\"file\"] = path\n",
    "        for t_i, text in enumerate(texts):\n",
    "            nlp_doc = grc_nlp.analyze(text=text) # piecewise, rather than a concat of the entire document, to save memory\n",
    "            sentences = nlp_doc.sentences\n",
    "            for s_i, s in enumerate(sentences):\n",
    "                metadata[\"sentence_idx\"] = f\"{t_i}_{s_i}\"\n",
    "                df_sents_raw.loc[len(df_sents_raw)] = s, metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {path}: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sents = df_sents_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "relevant_vars = [\"string\", \"pos\", \"lemma\", \"upos\", \"features\", \"category\", \"index_token\", \"governor\"]\n",
    "def extract_features(w: Word):\n",
    "    feats = {k:v for k,v in vars(w).items() if k in relevant_vars}\n",
    "    feats[\"pos\"] = str(feats[\"pos\"])\n",
    "    feats[\"features\"] = {str(k):str(v).strip(\"[]\") for k,v in feats[\"features\"].items()}\n",
    "    feats[\"category\"] = {str(k):str(v).strip(\"[]\") for k,v in feats[\"category\"].items()}\n",
    "    return feats\n",
    "df_sents[\"sentence_obj\"] = df_sents[\"sentence\"].apply(lambda s: [extract_features(w) for w in s.words])\n",
    "df_sents[\"sentence_txt\"] = df_sents[\"sentence\"].apply(sent_to_raw_text)\n",
    "latin_alphabet_indices = df_sents[\"sentence_txt\"].str.contains(r\"[A-Za-z]\")\n",
    "df_sents.loc[latin_alphabet_indices, \"sentence_txt\"] = df_sents[latin_alphabet_indices][\"sentence_txt\"]\\\n",
    "    .str.replace(f\"[A-Za-z0-9]\", \"\", regex=True)\\\n",
    "    .str.strip(string.printable) # drop latin characters, there are some mistakenly in a few places\n",
    "df_sents = df_sents.drop(columns=\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sents[\"len_words\"] = df_sents[\"sentence_obj\"].str.len()\n",
    "df_sents[\"len_chars\"] = df_sents[\"sentence_txt\"].str.len()\n",
    "df_sents = df_sents[df_sents[\"len_chars\"] >= 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sents.to_parquet(\"../data/sentences.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
